{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_similarity_metrics.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilkersigirci/ML-with-Colab/blob/master/text_similarity_metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbTyzA6PgnnC",
        "colab_type": "code",
        "outputId": "f6276ceb-784b-4db6-ba11-db422dbead56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from google.colab import drive\n",
        "from pprint import pprint\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRZj4OhxnIyb",
        "colab_type": "code",
        "outputId": "f30e06a6-3ee2-4789-c891-c362313e6560",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxwIqZwqH7Wx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cleaning text\n",
        "import re\n",
        "import nltk\n",
        "from string import punctuation\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "def pad_str(s):\n",
        "    return ' '+s+' '\n",
        "def pad_number(pattern):\n",
        "    matched_string = pattern.group(0)\n",
        "    return pad_str(matched_string)\n",
        "def pad_pattern(pattern):\n",
        "    matched_string = pattern.group(0)\n",
        "    return pad_str(matched_string)\n",
        "\n",
        "def clean(text):\n",
        "    \n",
        "    if pd.isnull(text):\n",
        "        return ''\n",
        "    \n",
        "    # Empty question   \n",
        "    if type(text) != str or text=='':\n",
        "        return ''\n",
        "    \n",
        "    # Clean the text, with the option to stem words.\n",
        "    # stops = set(stopwords.words(\"english\"))\n",
        "    \n",
        "    ###########################################################################################\n",
        "    ################################# SPECIFIC TO JOTFORM #####################################\n",
        "    ###########################################################################################\n",
        "    text = re.sub('<[^<]+?>', ' ',                  text, flags=re.IGNORECASE)  # remove html\n",
        "    text = re.sub('/(\\r\\n)+|\\r+|\\n+|\\t+/i', '' ,    text, flags=re.IGNORECASE)  # remove \\r \\n\n",
        "    text = re.sub(\"nbsp\", \"\",                       text, flags=re.IGNORECASE)  # remove nbsp\n",
        "    text = ''.join([c for c in text if c not in punctuation]).lower()           # remove punc\n",
        "    text = re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",     text, flags=re.IGNORECASE)  # remove tags\n",
        "    text = re.sub(\"http\\S*\", \"\",                    text, flags=re.IGNORECASE)  # remove links\n",
        "    text = re.sub(\"www\\S*\", \"\",                     text, flags=re.IGNORECASE)  # remove links\n",
        "    text = re.sub(\"\\S*@\\S*\\s?\", \"\",                 text, flags=re.IGNORECASE)  # remove email\n",
        "    text = re.sub(\"jo[a-z]*form\", \"jotform\",        text, flags=re.IGNORECASE)  # fix1 jotform\n",
        "    text = re.sub(\"jot[a-z]+\", \"jotform\",           text, flags=re.IGNORECASE)  # fix2 jotform\n",
        "    text = re.sub(\"jotform\", \"form\",                text, flags=re.IGNORECASE)  # jotform to form\n",
        "    text = re.sub(\"colour\", \"color\",                text, flags=re.IGNORECASE)  # fix colour\n",
        "    text = re.sub(\"authorizenet\", \"payment\",        text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"calender\", \"calendar\",           text, flags=re.IGNORECASE)\n",
        "    \n",
        "    text = re.sub(\"autosuspended\", \"auto suspend\",  text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"autoresponses\", \"auto response\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"wordpresscom\", \"word press\",     text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"myforms\", \"my form\",             text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"onedrive\", \"cloud\",              text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"gmailcom\", \"email\",              text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"jo[a-z]*form\", \"form\",           text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"unrequire\", \"not require\",       text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"prepopulate[a-z]*\", \"populate\",  text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"sumbissions\", \"submission\",      text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"submissons\", \"submission\",       text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"mozilla\\S*\", \"browser\",          text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"chrome\\S*\", \"browser\",           text, flags=re.IGNORECASE)\n",
        "    \n",
        "    \n",
        "    ###########################################################################################\n",
        "    ################################# SPECIFIC TO JOTFORM #####################################\n",
        "    ###########################################################################################\n",
        "    \n",
        "    #Fix Negative\n",
        "    text = re.sub(\"n't\", \" not \",                   text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"isnt\", \"is not\",                 text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"arent\", \"are not\",               text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"dont\", \"do not\",                 text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"doesnt\", \"does not\",             text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"didnt\", \"did not\",               text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"shouldnt\", \"should not\",         text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"hasnt\", \"has not\",               text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"havent\", \"have not\",             text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"wont\", \"will not\",               text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"cant\", \"can not\",                text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"couldnt\", \"could not\",           text, flags=re.IGNORECASE)\n",
        "    \n",
        "    text = re.sub(\"\\'s\", \" \",                       text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"whats\", \"what is\",               text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"\\'ve\", \" have \",                 text, flags=re.IGNORECASE)    \n",
        "    text = re.sub(\"i'm\", \"i am\",                    text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"\\'re\", \" are \",                  text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"\\'d\", \" would \",                 text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"\\'ll\", \" will \",                 text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"e\\.g\\.\", \" eg \",                 text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"e-mail\", \" email \",              text, flags=re.IGNORECASE)\n",
        "    text = re.sub('\\$', \" dollar \",                 text, flags=re.IGNORECASE)\n",
        "    text = re.sub('\\%', \" percent \",                text, flags=re.IGNORECASE)\n",
        "    text = re.sub('\\&', \" and \",                    text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"android\",\"operating system\",     text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"ios\", \"operating system\",        text, flags=re.IGNORECASE)\n",
        "    text = re.sub('[0-9]+\\.[0-9]+', \"flt_number\",   text, flags=re.IGNORECASE)  # replace the float numbers with flt_number    \n",
        "    text = re.sub(\" +\",\" \",                         text, flags=re.IGNORECASE)  # remove multiple space\n",
        "    text = re.sub('[^\\x00-\\x7F]+', \"\",              text)                       # remove non-ascii characters\n",
        "    \n",
        "    ############################### LEMMATIZE ####################################\n",
        "    \n",
        "    from nltk import word_tokenize, pos_tag\n",
        "    from nltk.stem.wordnet import WordNetLemmatizer\n",
        "    #nltk.download('wordnet')\n",
        "    #nltk.download('punkt')\n",
        "    #nltk.download('averaged_perceptron_tagger')\n",
        "    # n -> noun / v -> verb / a -> adjective / r -> adverb\n",
        "    tokens = word_tokenize(text)\n",
        "    convert_tag = lambda e: ('a' if e[0].lower() == 'j' else e[0].lower()) if e[0].lower() in ['n', 'r', 'v'] else 'n'    \n",
        "    lemmatizer = WordNetLemmatizer()    \n",
        "    text  = ' '.join([ lemmatizer.lemmatize(token, convert_tag(tag)) for token,tag in pos_tag(tokens) ])\n",
        "    \n",
        "    ############################### STOP WORDS ####################################\n",
        "    #nltk.download('stopwords')\n",
        "    from nltk.corpus import stopwords\n",
        "    stop_words = stopwords.words('english')\n",
        "    text =  \" \".join(word for word in text.split() if word not in stop_words)\n",
        "\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Zrm024_dZ3E",
        "colab_type": "code",
        "outputId": "7b1380bc-ff8f-4909-e1f1-b0491ff20d3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "nltk.download('punkt')\n",
        "\n",
        "def initial_clean(text):\n",
        "    \"\"\"\n",
        "    Function to clean text of websites, email addresess and any punctuation\n",
        "    We also lower case the text\n",
        "    \"\"\"\n",
        "    text = text.lower() # lower case the text\n",
        "    text = re.sub(\"((\\S+)?(http(s)?)(\\S+))|((\\S+)?(www)(\\S+))|((\\S+)?(\\@)(\\S+)?)\", \" \", text)\n",
        "    text = re.sub(\"[^a-zA-Z ]\", \"\", text)    \n",
        "    text = ''.join([c for c in text if c not in punctuation]).lower()           # remove punc\n",
        "    text = re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",     text, flags=re.IGNORECASE)  # remove tags\n",
        "    text = nltk.word_tokenize(text)\n",
        "    return text\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "def remove_stop_words(text):\n",
        "    \"\"\"\n",
        "    Function that removes all stopwords from text\n",
        "    \"\"\"\n",
        "    return [word for word in text if word not in stop_words]\n",
        "\n",
        "#stemmer = PorterStemmer()\n",
        "def stem_words(text):\n",
        "    \"\"\"\n",
        "    Function to stem words, so plural and singular are treated the same\n",
        "    \"\"\"\n",
        "    try:\n",
        "        text = [stemmer.stem(word) for word in text]\n",
        "        text = [word for word in text if len(word) > 1] # make sure we have no 1 letter words\n",
        "    except IndexError: # the word \"oed\" broke this, so needed try except\n",
        "        pass\n",
        "    return text\n",
        "\n",
        "def apply_all(text):\n",
        "    \"\"\"\n",
        "    This function applies all the functions above into one\n",
        "    \"\"\"\n",
        "    #return stem_words(remove_stop_words(initial_clean(text)))\n",
        "    return remove_stop_words(initial_clean(text))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLXkdsJAXILU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir mydata"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROsDIk5eW6tj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### ORIGINAL CSV #########################################\n",
        "\n",
        "!cp drive/My\\ Drive/Colab\\ Notebooks/Dataset/jotform/original.csv  mydata/\n",
        "\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "dfPath = 'mydata/original.csv'\n",
        "original = pd.read_csv(dfPath, sep='|',index_col=0)\n",
        "\n",
        "#original = original.dropna(axis = 0)\n",
        "#original[\"id\"] = original[\"id\"].apply(str)\n",
        "\n",
        "#original_questions = original['question']\n",
        "#original_questions = np.array(original_questions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMREBDnu5HSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### PREPROCESSED CSV ####################################\n",
        "\n",
        "!cp drive/My\\ Drive/Colab\\ Notebooks/Dataset/jotform/preprocessed.csv  mydata/\n",
        "\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "dfPath = 'mydata/preprocessed.csv'\n",
        "df = pd.read_csv(dfPath, sep='|',index_col=0)\n",
        "\n",
        "questions = df['all']\n",
        "questions = np.array(questions)\n",
        "original_all = df['question'] + \"  / \" + df[\"details\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRWqFEIM6RnY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split sentences\n",
        "splitted = []\n",
        "for sentence in questions:\n",
        "    if type(sentence) == str:\n",
        "        splitted.append(sentence.lower().split())\n",
        "\n",
        "splitted = np.array(splitted)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGZZEt0Dvk7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################################## TF-IDF ############################################## \n",
        "import nltk, string, numpy\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stemmer = nltk.stem.porter.PorterStemmer()\n",
        "def StemTokens(tokens):\n",
        "    return [stemmer.stem(token) for token in tokens]\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "def StemNormalize(text):\n",
        "    return StemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
        "\n",
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "def LemTokens(tokens):\n",
        "    return [lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
        "\n",
        "###### TF-IDF HARD WAY (ram yetmiyor) ######## \n",
        "\"\"\"\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "LemVectorizer = CountVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "LemVectorizer.fit_transform(documents)\n",
        "print (LemVectorizer.vocabulary_)\n",
        "\n",
        "tf_matrix = LemVectorizer.transform(documents).toarray() # out of ram error\n",
        "print(tf_matrix)\n",
        "\n",
        "# Inverse Document Freq (idf)\n",
        "# terim ne kadar az dökumanda tekrar ediyor ise IDF değeri o kadar büyük çıkar.\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "    tfidfTran = TfidfTransformer(norm=\"l2\") # euclidian distance\n",
        "    tfidfTran.fit(tf_matrix)\n",
        "    print tfidfTran.idf_\n",
        "    \n",
        "tfidf_matrix = tfidfTran.transform(tf_matrix)\n",
        "print tfidf_matrix.toarray()\n",
        "\"\"\"\n",
        "###### TF-IDF EASY WAY ########\n",
        "# TfidfVectorizer combines the work of CountVectorizer and TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer() # (tokenizer=LemNormalize, stop_words='english')\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "# TF-IDF matrix for each document 4 -> (the number of rows of the matrix) \n",
        "# Tf-idf terms                    11-> (the number of columns from the matrix)\n",
        "print (tfidf_matrix.shape)\n",
        "print(tfidf_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb0qAAbQxnBt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Cosine similarity with tf-idf\n",
        "\"\"\"\n",
        "def cos_similarity(textlist):\n",
        "    tfidf = TfidfVec.fit_transform(textlist)\n",
        "    return (tfidf * tfidf.T).toarray()\n",
        "cos_similarity(documents)\n",
        "\"\"\"\n",
        "#Better way\n",
        "#Cosine Similarity between the first document with all documents in the set\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "cosine_similarity(tfidf_matrix[0], tfidf_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWpeK9RI8zLk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################### FuzzyWuzzy ############################################## \n",
        "#!pip install fuzzywuzzy\n",
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "# ratio            -> 51 -> compares the entire string similarity, in order\n",
        "# partial_ratio    -> 52 -> compares partial string similarity.\n",
        "# token_sort_ratio -> 46 -> ignores word order.\n",
        "# token_set_ratio  -> 52 -> ignores duplicated words\n",
        "\n",
        "print(fuzz.partial_token_set_ratio(documents[0], documents[1])) # 100\n",
        "print(fuzz.partial_token_set_ratio(documents[0], documents[2])) # 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DN_iMGU5dFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################### Word2Vec ############################################## \n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# size      -> determines the dimensions of the word vectors\n",
        "# window    -> number of forward and backward words used in training\n",
        "# min_count -> number of minimum times a word needs to appear to be included in the training\n",
        "# sg        -> 0 = CBOW / 1 = skip gram\n",
        "\n",
        "w2v_model      = Word2Vec(splitted, size = 100, window = 5, min_count = 3) # 0.62 min\n",
        "w2v_model_sg2  = Word2Vec(splitted, size = 100, window = 5, min_count = 3,sg = 2) # 1.89 min\n",
        "w2v_model_norm = Word2Vec(splitted, size = 100, window = 5, min_count = 3) # 0.60 min\n",
        "w2v_model_norm.init_sims(replace=True) #normalize word2vec vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8092csnoI_l",
        "colab_type": "code",
        "outputId": "da640251-465e-4de9-abd2-d968fcd234e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "w2v_model.save('mydata/w2v_model_size100_window5_min3.model')\n",
        "w2v_model_sg2.save('mydata/w2v_model_sg2_size100_window5_min3.model')\n",
        "w2v_model_norm.save('mydata/w2v_model_norm_size100_window5_min3.model')\n",
        "!cp mydata/w2v_model_size100_window5_min3.model drive/My\\ Drive/Colab\\ Notebooks/Dataset/jotform/models/\n",
        "!cp mydata/w2v_model_sg2_size100_window5_min3.model drive/My\\ Drive/Colab\\ Notebooks/Dataset/jotform/models/\n",
        "!cp mydata/w2v_model_norm_size100_window5_min3.model drive/My\\ Drive/Colab\\ Notebooks/Dataset/jotform/models/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjfiXw82pW3N",
        "colab_type": "code",
        "outputId": "efeb1dae-641c-4bcf-aa5a-1bb8e74b0327",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "#Save model\n",
        "#w2v_model.save('mydata/w2v_model_size100_window5_min3.model')\n",
        "#!cp mydata/w2v_model_size100_window5_min3.model drive/My\\ Drive/Colab\\ Notebooks/Dataset/jotform/models/\n",
        "\n",
        "#Load model\n",
        "!cp drive/My\\ Drive/Colab\\ Notebooks/Dataset/jotform/models/w2v_model_size100_window5_min3.model mydata/\n",
        "!cp drive/My\\ Drive/Colab\\ Notebooks/Dataset/jotform/models/w2v_model_sg2_size100_window5_min3.model mydata/\n",
        "!cp drive/My\\ Drive/Colab\\ Notebooks/Dataset/jotform/models/w2v_model_norm_size100_window5_min3.model mydata/\n",
        "w2v_model      = Word2Vec.load('mydata/w2v_model_size100_window5_min3.model')\n",
        "w2v_model_sg2  = Word2Vec.load('mydata/w2v_model_sg2_size100_window5_min3.model')\n",
        "w2v_model_norm = Word2Vec.load('mydata/w2v_model_norm_size100_window5_min3.model')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpItO0SvRz_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################### Glove ############################################## \n",
        "!wget http://nlp.stanford.edu/data/glove.840B.300d.zip -P mydata/\n",
        "!unzip mydata/glove.840B.300d.zip -d mydata/\n",
        "!touch mydata/glove_word2vec.txt\n",
        "\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "_ = glove2word2vec(r'mydata/glove.840B.300d.txt', r\"mydata/glove_word2vec.txt\")\n",
        "##########################################################\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "pre_model = KeyedVectors.load_word2vec_format(r\"mydata/glove_word2vec.txt\", binary=False)\n",
        "model_glove = gensim.models.Word2Vec(size=100, min_count=5)\n",
        "model_glove.build_vocab(splitted)\n",
        "total_examples = model_glove.corpus_count\n",
        "model_glove.build_vocab([list(pre_model.vocab.keys())], update=True)\n",
        "model_glove.intersect_word2vec_format(r\"mydata/glove_word2vec.txt\", binary=False, lockf=1.0)\n",
        "model_glove.train(tokenized, total_examples=total_examples, epochs=model_glove.iter)\n",
        "##########################################################\n",
        "print(model_glove.most_similar(positive=['blue'], topn=10))\n",
        "print(w2v_model.most_similar(positive=['blue'], topn=10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "597IA2HyZjoA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###################################### WMD Distance ###########################################\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "#nltk.download('stopwords')\n",
        "#stop_words = stopwords.words('english')\n",
        "\n",
        "def my_wmd(q1, q2,model_type):\n",
        "    q1 = str(q1).lower().split()\n",
        "    q2 = str(q2).lower().split()\n",
        "    #q1 = [w for w in q1 if w not in stop_words]\n",
        "    #q2 = [w for w in q2 if w not in stop_words]\n",
        "    return model_type.wv.wmdistance(q1, q2)\n",
        "\n",
        "#all_except_index = np.delete(a, ind, axis=0) \n",
        "\n",
        "t1 = time.time()\n",
        "\n",
        "text = \"how to change font in google forms\"\n",
        "my_question = clean(text)\n",
        "\n",
        "wmd_distance = []\n",
        "for question in questions:\n",
        "    wmd_distance.append(my_wmd(my_question,question,w2v_model))\n",
        "wmd_distance = np.array(wmd_distance)\n",
        "\n",
        "t2 = time.time()\n",
        "print(\"Calculation took \",(t2-t1)/60, \"min\") # 12\n",
        "\n",
        "\"\"\"\n",
        "print(\"wmd normalize starting..\")\n",
        "wmd_distance_norm = []\n",
        "for question in questions:\n",
        "    wmd_distance_norm.append(my_wmd(my_question,question,w2v_model_norm))\n",
        "wmd_distance_norm = np.array(wmd_distance_norm)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrNRPfY0amKq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the indices of the four min elements -> returns unsorted\n",
        "min_indexes = np.argpartition(wmd_distance, 10)[:10]\n",
        "min_indexes = min_indexes[np.argsort(wmd_distance[min_indexes])] #sort it\n",
        "#min_indexes_norm = np.argpartition(wmd_distance_norm, 4)[:4]\n",
        "#min_indexes_norm = min_indexes_norm[np.argsort(wmd_distance_norm[min_indexes_norm])] #sort it\n",
        "\n",
        "print(\"question             \\n {}\".format(text))\n",
        "print(\"similar questions       \\n {}\".format(df['question'][min_indexes] + \" \" + df['details'][min_indexes]))\n",
        "#print(\"similar questions norm  \\n {}\".format(questions[min_indexes_norm]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwCIhRZakLku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def combine_wmd(my_question,similar,model_type,number): # questionlar string formda olacak\n",
        "    t1 = time.time()\n",
        "    \n",
        "    wmd_distance = []\n",
        "    for question in similar['all']:\n",
        "        distance = model_type.wv.wmdistance(my_question.split(), question.split())        \n",
        "        wmd_distance.append(distance)\n",
        "        \n",
        "    wmd_distance = np.array(wmd_distance)\n",
        "    min_indexes = np.argpartition(wmd_distance, number)[:number]\n",
        "    min_indexes = min_indexes[np.argsort(wmd_distance[min_indexes])]\n",
        "    t2 = time.time()\n",
        "    print(\"Calculation took \",t2-t1, \"seconds\")\n",
        "    \n",
        "    return min_indexes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIEvT-VFdy-Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###################################### WMD Similarity ###########################################\n",
        "\n",
        "from gensim.similarities import WmdSimilarity\n",
        "instance = WmdSimilarity(splitted, w2v_model, num_best=10)\n",
        "\n",
        "t1 = time.time()\n",
        "\n",
        "my_question = \"How do i retrieve the forms I created prior to upgrading?\"\n",
        "my_question = clean(my_question)\n",
        "\n",
        "sims = instance[my_question]  # A query is simply a \"look-up\" in the similarity class.\n",
        "\n",
        "t2 = time.time()\n",
        "print(\"Calculation took \",(t2-t1)/60, \"min\")  # Took 28 min\n",
        "\n",
        "# Print the query and the retrieved documents, together with their similarities.\n",
        "print ('Query:')\n",
        "print (my_question)\n",
        "for i in range(10):\n",
        "    print\n",
        "    print ('sim = %.4f' % sims[i][1])\n",
        "    print (questions[sims[i][0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB6pWLNio4E8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################### Sent2Vec ##############################################\n",
        "\n",
        "from gensim.models import Sent2Vec\n",
        "\n",
        "s2v_model      = Sent2Vec(splitted, size = 100, window = 5, min_count = 1)\n",
        "s2v_model_norm = Sent2Vec(splitted, size = 100, window = 5, min_count = 1)\n",
        "s2v_model_norm.init_sims(replace=True) #normalize word2vec vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdt2V_gpKHss",
        "colab_type": "code",
        "outputId": "34127859-ebc1-4b47-ffb4-0a57307cc8bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "\n",
        "def sent2vec(s):\n",
        "    words = str(s).lower()\n",
        "    words = word_tokenize(words)\n",
        "    words = [w for w in words if not w in stop_words]\n",
        "    words = [w for w in words if w.isalpha()]\n",
        "    M = []\n",
        "    for w in words:\n",
        "        try:\n",
        "            M.append(w2v_model[w])\n",
        "        except:\n",
        "            continue\n",
        "    M = np.array(M)\n",
        "    print(M)\n",
        "    v = M.sum(axis=0)\n",
        "    return v / np.sqrt((v ** 2).sum())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qi_dLx4TKN0s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Ram patliyor\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "question_vectors = np.zeros((df.shape[0], 10))\n",
        "\n",
        "for i, q in enumerate(tqdm_notebook(questions)):\n",
        "    question_vectors[i, :] = sent2vec(q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_nTtsJoLkVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cosine_distance = [cosine(x, y) for q1_vec in np.nan_to_num(question_vectors)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZwo9g1aqEKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################### Doc2Vec ##############################################\n",
        "from gensim.models import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "\n",
        "tagged_questions = [TaggedDocument(doc, [i]) for i, doc in enumerate(splitted)]\n",
        "#d2v_model = Doc2Vec(tagged_questions, size = 100, window = 10, min_count = 1)\n",
        "\n",
        "\n",
        "# dm=1         -> distributed Memory(PV-DM)  -> preserves word order -> same as CBOW\n",
        "# dm=0         -> distributed BOW   (PV-DBOW)-> same as skipgram\n",
        "# dbow_words=1 -> add skip-gram word-training\n",
        "# hs           -> hierarchical-softmax, better but slower / default is disable\n",
        "d2v_model = Doc2Vec(dm = 0, min_count=2, window=10, size=100, sample=1e-4, negative=5,iter=10,workers=4)\n",
        "d2v_model.build_vocab(tagged_questions)\n",
        "\n",
        "for epoch in range(20):\n",
        "    d2v_model.train(tagged_questions,total_examples=d2v_model.corpus_count, epochs=d2v_model.iter)\n",
        "    print(\"Epoch #{} is complete.\".format(epoch+1))\n",
        "    \n",
        "# Store the model to mmap-able files\n",
        "#d2v_model.save('d2v_dm0_epoch10.model')\n",
        "# Load the model\n",
        "#d2v_model = Doc2Vec.load('d2v_dm0_epoch10.model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5n3LMBlPPOXX",
        "colab_type": "code",
        "outputId": "1d53661e-0188-49e5-d1be-a83a0f693ffc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#Save model\n",
        "#d2v_model.save('mydata/d2v_dm0_epoch10.model')\n",
        "#!cp mydata/d2v_dm0_epoch10.model drive/My\\ Drive/Colab\\ Notebooks/Dataset/jotform/\n",
        "#!cp mydata/d2v_dm0_epoch10.model.docvecs.vectors_docs.npy drive/My\\ Drive/Colab\\ Notebooks/Dataset/jotform/models/\n",
        "\n",
        "#Load model\n",
        "from gensim.models import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "\n",
        "!cp drive/My\\ Drive/Colab\\ Notebooks/Dataset/jotform/models/d2v_dm0_epoch10.model mydata/\n",
        "!cp drive/My\\ Drive/Colab\\ Notebooks/Dataset/jotform/models/d2v_dm0_epoch10.model.docvecs.vectors_docs.npy mydata/\n",
        "d2v_model = Doc2Vec.load('mydata/d2v_dm0_epoch10.model')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ofpeuj6TzHSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"Please cancel my subscription and refund the amount.\"\n",
        "\n",
        "\n",
        "def doc2vec_similar(text):\n",
        "    my_question = preprocess(text) # splitted formda\n",
        "    my_question = \" \".join(my_question)\n",
        "    \n",
        "    new_vector = d2v_model.infer_vector(my_question.split(), epochs=50, alpha=0.25)\n",
        "    #gives you top k document tags and their cosine similarity\n",
        "    similars = d2v_model.docvecs.most_similar([new_vector],topn=7000)\n",
        "\n",
        "    similar_index = [index[0] for index in similars]\n",
        "    d2v_similar = df.loc[similar_index]\n",
        "    d2v_similar.reset_index(drop=True,inplace=True)\n",
        "\n",
        "    # Combine d2v with WMD\n",
        "    min_indexes_d2v = combine_wmd(my_question,d2v_similar,w2v_model,10)\n",
        "\n",
        "    print(\"question             \\n {}\".format(text))\n",
        "    print(\"similar questions    \\n {}\".format(d2v_similar['question'][min_indexes_d2v] + \" / \" + d2v_similar['details'][min_indexes_d2v]))\n",
        "\n",
        "#score = d2v_model.n_similarity(questions[1],questions[2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ba30EMyznc9P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################### LDA ##############################################\n",
        "\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "import numpy as np\n",
        "np.random.seed(7)\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Plotting tools\n",
        "!pip install pyLDAvis\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim  # don't skip this\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlMxw0wtwLxo",
        "colab_type": "code",
        "outputId": "08d7aa8e-fe2c-4c7e-b545-1a700d52438f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "data_questions = df[[\"all\"]]\n",
        "data_questions['index'] = data_questions.index\n",
        "questions = data_questions"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPzlSXaMxViZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "def lemmatize_stemming(text):\n",
        "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
        "\n",
        "def preprocess(text):\n",
        "    result = []\n",
        "    for token in text.split():\n",
        "        token = clean(token)\n",
        "        if len(token) > 2:\n",
        "            #result.append(lemmatize_stemming(token))\n",
        "            result.append(token)\n",
        "        \n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQYTqZIQx_t0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processed_questions = splitted\n",
        "np.random.seed(7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQld_CQWxch4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################ Bag of words on the dataset ############################\n",
        "dictionary = gensim.corpora.Dictionary(processed_questions)\n",
        "\n",
        "#less than 15 documents (absolute number) or\n",
        "#more than 0.5 documents (fraction of total corpus size, not absolute number).\n",
        "#keep only the first 100000 most frequent tokens.\n",
        "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=70000)\n",
        "\n",
        "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_questions]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mYKXRao0GCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####################### TF-IDF ############################\n",
        "\n",
        "from gensim import corpora, models\n",
        "\n",
        "tfidf = models.TfidfModel(bow_corpus)\n",
        "corpus_tfidf = tfidf[bow_corpus]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BNP8NOx1C_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########### Running LDA using Bag of Words #################\n",
        "np.random.seed(7) # for getting same results each time.\n",
        "t1 = time.time()\n",
        "\n",
        "lda_model  = gensim.models.LdaMulticore(bow_corpus, num_topics=100, id2word=dictionary, passes=5, workers=4) # 0.45\n",
        "lda_model_2 = gensim.models.LdaMulticore(bow_corpus, num_topics=150, id2word=dictionary, passes=5, workers=4) # 0.45\n",
        "\n",
        "t2 = time.time()\n",
        "print(\"Calculation took \",(t2-t1)/60, \"min\")  # Took 10 min"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qw3njJh9G2z1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pprint(lda_model.print_topics())\n",
        "for idx, topic in lda_model_2.print_topics(-1):\n",
        "    print('Topic: {} Words: {}'.format(idx, topic))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTEmCudIsWli",
        "colab_type": "code",
        "outputId": "2bc7af98-7719-4d63-d080-74fbaef5e310",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from gensim.models.ldamulticore import LdaMulticore\n",
        "\n",
        "#Save model\n",
        "#lda_model.save(\"mydata/lda_model_topic150_pass5.model\")\n",
        "#!cp mydata/lda_model_topic150_pass5.model                 drive/My\\ Drive/Colab\\ Notebooks/Dataset/jotform/models/lda/\n",
        "#!cp mydata/lda_model_topic150_pass5.model.expElogbeta.npy drive/My\\ Drive/Colab\\ Notebooks/Dataset/jotform/models/lda/\n",
        "#!cp mydata/lda_model_topic150_pass5.model.id2word         drive/My\\ Drive/Colab\\ Notebooks/Dataset/jotform/models/lda/\n",
        "#!cp mydata/lda_model_topic150_pass5.model.state           drive/My\\ Drive/Colab\\ Notebooks/Dataset/jotform/models/lda/\n",
        "\n",
        "#Load model\n",
        "\n",
        "!cp drive/My\\ Drive/Colab\\ Notebooks/Dataset/jotform/models/lda/lda_model_topic150_pass5.model                   mydata/\n",
        "!cp drive/My\\ Drive/Colab\\ Notebooks/Dataset/jotform/models/lda/lda_model_topic150_pass5.model.expElogbeta.npy   mydata/\n",
        "!cp drive/My\\ Drive/Colab\\ Notebooks/Dataset/jotform/models/lda/lda_model_topic150_pass5.model.id2word           mydata/\n",
        "!cp drive/My\\ Drive/Colab\\ Notebooks/Dataset/jotform/models/lda/lda_model_topic150_pass5.model.state             mydata/\n",
        "\n",
        "!cp drive/My\\ Drive/Colab\\ Notebooks/Dataset/jotform/models/lda/lda_model_2_topic150_pass5.model                 mydata/\n",
        "!cp drive/My\\ Drive/Colab\\ Notebooks/Dataset/jotform/models/lda/lda_model_2_topic150_pass5.model.expElogbeta.npy mydata/\n",
        "!cp drive/My\\ Drive/Colab\\ Notebooks/Dataset/jotform/models/lda/lda_model_2_topic150_pass5.model.id2word         mydata/\n",
        "!cp drive/My\\ Drive/Colab\\ Notebooks/Dataset/jotform/models/lda/lda_model_2_topic150_pass5.model.state           mydata/\n",
        "\n",
        "lda_model   = LdaMulticore.load('mydata/lda_model_topic150_pass5.model')\n",
        "lda_model_2 = LdaMulticore.load('mydata/lda_model_2_topic150_pass5.model')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tckL-p2Q1GmA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########### Running LDA using TF-IDF #################\n",
        "t1 = time.time()\n",
        "\n",
        "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=100, id2word=dictionary, passes=5, workers=4)\n",
        "lda_model_tfidf.save(\"mydata/lda_model_tfidf_topic100_pass5.model\")\n",
        "t2 = time.time()\n",
        "print(\"Calculation took \",(t2-t1)/60, \"min\")  # Took 8 min\n",
        "\n",
        "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
        "    print('Topic: {} Word: {}'.format(idx, topic))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUAalJL7oMRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize lda model\n",
        "\n",
        "# Fairly big, non-overlapping bubbles instead of being clustered in one quadrant.  -> GOOD MODEL\n",
        "# Too many topics typically have many overlaps, small bubbles clustered in one region.\n",
        "\n",
        "#!pip install pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "#import pyLDAvis.sklearn\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "display_lda =  pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary)\n",
        "#pyLDAvis.display(display_lda)\n",
        "display_lda"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A2OAweuwZFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################ LDA MALLET ###############################\n",
        "!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip -P mydata/\n",
        "!unzip mydata/mallet-2.0.8.zip -d mydata/\n",
        "mallet_path = 'mydata/mallet-2.0.8/bin/mallet' # update this path\n",
        "\n",
        "t1 = time.time()\n",
        "lda_model = gensim.models.wrappers.LdaMallet(mallet_path, bow_corpus, num_topics=100, id2word=dictionary, workers=3) #, passes=5\n",
        "t2 = time.time()\n",
        "print(\"Calculation took \",(t2-t1)/60, \"min\")  # Took min"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgfi1pVKrZRs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################ TESTING MODEL COHERENCE ###########################3\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# topic100        = 0.45\n",
        "# topic100_mallet = 0.51\n",
        "# topic50_mallet  = 0.57\n",
        "\n",
        "# Compute Coherence Score\n",
        "#coherence_model_lda = CoherenceModel(model=lda_model, texts=splitted, dictionary=dictionary, coherence='c_v')\n",
        "#coherence_lda = coherence_model_lda.get_coherence()\n",
        "#print('\\nCoherence Score: ', coherence_lda) # 0.51\n",
        "\n",
        "def compute_coherence_values(dictionary, corpus, texts, limit, start=50, step=50):\n",
        "\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=dictionary,workers=4)\n",
        "        print(\"model {} is finished\".format(num_topics))\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        print(\"coherence {} is finished\".format(num_topics))\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "\n",
        "    return model_list, coherence_values\n",
        "\n",
        "t1 = time.time()\n",
        "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=bow_corpus,  \n",
        "                                                        texts=splitted, start=20, limit=60, step=20)\n",
        "t2 = time.time()\n",
        "print(\"Calculation took \",(t2-t1)/60, \"min\")  # Took 23 min\n",
        "\n",
        "\n",
        "# Show graph\n",
        "limit=60; start=20; step=20;\n",
        "x = range(start, limit, step)\n",
        "plt.plot(x, coherence_values)\n",
        "plt.xlabel(\"Num Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence_values\"), loc='best')\n",
        "plt.show()\n",
        "\n",
        "# Print the coherence scores\n",
        "for m, cv in zip(x, coherence_values):\n",
        "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3wdq_SPEeBd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################ DISTANCE TRIALS #################################\n",
        "\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "from gensim.matutils import kullback_leibler, jaccard, hellinger, sparse2full, cossim, softcossim\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def distance_computations(dataframe,all_lda_corpus,que_lda,distance_metric,number,select):    \n",
        "\n",
        "    t1 = time.time()\n",
        "    distances = []\n",
        "    for each_que_lda in all_lda_corpus:\n",
        "        distances.append( distance_metric(que_lda, each_que_lda) )\n",
        "\n",
        "    distances = np.array(distances)\n",
        "    # Get the indices of the number max-min elements -> returns unsorted\n",
        "    if(select == \"min\")  :\n",
        "        indexes = np.argpartition(distances, number)[:number]\n",
        "        indexes = indexes[np.argsort(distances[indexes])] #sort it -> ascending\n",
        "    elif(select == \"max\"): \n",
        "        indexes = np.argpartition(distances, -number)[-number:] \n",
        "        indexes = indexes[np.argsort(distances[indexes])][::-1] #sort it -> descending\n",
        "\n",
        "    similar = dataframe.loc[indexes]\n",
        "    similar.reset_index(drop=True,inplace=True)\n",
        "    t2 = time.time()\n",
        "    print(\"Calculation took \",t2-t1, \"seconds\")\n",
        "    \n",
        "    my_print(similar,indexes,distance_metric,\"original\")\n",
        "    \n",
        "    return similar\n",
        "\n",
        "\n",
        "def my_print(similar,indexes,distance_metric,select):\n",
        "    print(\"####################################################################################################\")\n",
        "    \n",
        "    if(select == \"original\"):\n",
        "        print(\"similar questions -> {}    \\n {}\".format( distance_metric, original_all[indexes][:10] ))\n",
        "    elif(select == \"similar\"):   \n",
        "        print(\"similar questions    \\n {}\".format(similar['question'][indexes] + \" / \" + similar['details'][indexes]))\n",
        "    \n",
        "    print(\"####################################################################################################\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1Ohpl8O0JJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#questions_final = df['all']\n",
        "#questions_final = np.array(questions_final)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjk13a5tFB8y",
        "colab_type": "code",
        "outputId": "cb2e022a-8233-4777-e60c-b79de5dadac5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "t1 = time.time()\n",
        "all_lda = [lda_model[x] for x in bow_corpus]\n",
        "all_lda_2 = [lda_model_2[x] for x in bow_corpus]\n",
        "t2 = time.time()\n",
        "print(\"Calculation took \",(t2-t1)/60, \"min\")  # Took 5 min"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculation took  6.963758293787638 min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6eVMXRK4Wnm",
        "colab_type": "code",
        "outputId": "c2f76138-f0fd-4300-b478-853fd85ebddc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "#text = \"Is it possible to transfer a form to another jotform account?\"\n",
        "#text = \"Please cancel my subscription and refund the amount.\"\n",
        "#text = \"I am unable to free up the upload space even after deleting submissions.\"\n",
        "text = \"Can i see the time user spend to fill the form\"\n",
        "#text = \"Unable to view photos upon submission of form\"\n",
        "#text = \"One column not wide enough and other is too wide\"\n",
        "#text = \"How to create conditional thank you page?\"\n",
        "#text = \"Allow multiple date selection and to not lock the selected dates\"\n",
        "que_question = preprocess(text) # splitted formda\n",
        "que_question = preprocess(text) # splitted formda\n",
        "print(que_question)\n",
        "#que_question = splitted[100]\n",
        "#que_bow = [dictionary.doc2bow(que_question)]  # dogrusu bu gibi ama hata veriyor\n",
        "que_bow   = dictionary.doc2bow(que_question)\n",
        "que_lda   = lda_model[que_bow]\n",
        "que_lda_2 = lda_model_2[que_bow]"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['see', 'time', 'user', 'spend', 'fill', 'form']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4hr_ncpuJCF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc2vec_similar(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GxlivZMIM_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hellinger\n",
        "\n",
        "print(\"my_question             \\n {}\".format(text))\n",
        "similar_hellinger   = distance_computations(df, all_lda,   que_lda,   hellinger, 7000, \"min\")\n",
        "similar_2_hellinger = distance_computations(df, all_lda_2, que_lda_2, hellinger, 7000, \"min\")\n",
        "\n",
        "# Combine hellinger with WMD\n",
        "que_question = \" \".join(que_question)\n",
        "\n",
        "print(\"question             \\n {}\".format(text))\n",
        "min_indexes_wmd = combine_wmd(que_question,similar_hellinger,w2v_model,10)\n",
        "my_print(similar_hellinger,min_indexes_wmd,hellinger,\"similar\")\n",
        "min_indexes_2_wmd = combine_wmd(que_question,similar_2_hellinger,w2v_model,10)\n",
        "my_print(similar_2_hellinger,min_indexes_2_wmd,hellinger,\"similar\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piNJxQiF5zSG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cosine\n",
        "\n",
        "print(\"my_question             \\n {}\".format(text))\n",
        "similar_cos   = distance_computations(df, all_lda,   que_lda,   cossim, 7000,\"max\")\n",
        "similar_2_cos = distance_computations(df, all_lda_2, que_lda_2, cossim, 7000,\"max\")\n",
        "\n",
        "# Combine hellinger with WMD\n",
        "que_question = \" \".join(que_question)\n",
        "\n",
        "print(\"question             \\n {}\".format(text))\n",
        "min_indexes_wmd = combine_wmd(que_question,similar_cos,w2v_model,10)\n",
        "my_print(similar_cos,min_indexes_wmd,cossim,\"similar\")\n",
        "min_indexes_2_wmd = combine_wmd(que_question,similar_2_cos,w2v_model,10)\n",
        "my_print(similar_2_cos,min_indexes_2_wmd,cossim,\"similar\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzcPvSax2bki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# kullback_leibler #not symmetric       ->  hata veriyor bazen\n",
        "\n",
        "kullback_leibler_distance = []\n",
        "for each_que_lda in all_lda:\n",
        "    kullback_leibler_distance.append( kullback_leibler(que_lda, each_que_lda) )\n",
        "    \n",
        "kullback_leibler_distance = np.array(kullback_leibler_distance)\n",
        "\n",
        "# Get the indices of the four min elements -> returns unsorted\n",
        "min_indexes = np.argpartition(kullback_leibler_distance, 4)[:4]\n",
        "min_indexes = min_indexes[np.argsort(kullback_leibler_distance[min_indexes])] #sort it\n",
        "\n",
        "print(\"my_question             \\n {}\".format(que_question))\n",
        "print(\"similar questions       \\n {}\".format(questions_final[min_indexes]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZQ3TEBR6fwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# jensenshannon\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "from gensim.matutils import kullback_leibler, jaccard, hellinger, sparse2full\n",
        "\n",
        "shanjen_distance = []\n",
        "for each_que_lda in all_lda:\n",
        "    shanjen_distance.append( jensenshannon(que_lda, each_que_lda) )\n",
        "    \n",
        "shanjen_distance = np.array(shanjen_distance)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMtBHzk8wl8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##################################################################################################################################################################################################################\n",
        "###################################################################################### END #######################################################################################################################\n",
        "##################################################################################################################################################################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-LkdcP0Hcbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#################### jensenshannon manual ##############################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVoLqi5RGIJv",
        "colab_type": "code",
        "outputId": "e311fcc9-d3fb-41c2-e85f-037cb7893050",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "que_question = processed_questions[80]\n",
        "new_bow = dictionary.doc2bow(que_question)\n",
        "\n",
        "t1 = time.time()\n",
        "new_doc_distribution = np.array([tup[1] for tup in lda_model.get_document_topics(bow=new_bow)])\n",
        "t2 = time.time()\n",
        "#print(\"Calculation took \",(t2-t1)/60, \"min\")  # Took 4 min\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculation took  3.689130147298177e-05 min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUuyULhuG7-T",
        "colab_type": "code",
        "outputId": "9d523f5d-f8e4-4c7a-8b05-575e87db9547",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# we need to use nested list comprehension here\n",
        "t1 = time.time()\n",
        "doc_topic_dist = np.array([[tup[1] for tup in lst] for lst in lda_model[bow_corpus]])\n",
        "t2 = time.time()\n",
        "print(\"Calculation took \",(t2-t1)/60, \"min\")  # Took 2 min"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculation took  2.2370501160621643 min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ypIXQQvXcC6",
        "colab_type": "code",
        "outputId": "e5310480-2873-46ab-eefd-bc20c46f13ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "doc_topic_dist.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(218519,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEaF7d-cHGNY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def jensen_shannon(query, matrix):\n",
        "    \"\"\"\n",
        "    This function implements a Jensen-Shannon similarity\n",
        "    between the input query (an LDA topic distribution for a document)\n",
        "    and the entire corpus of topic distributions.\n",
        "    It returns an array of length M where M is the number of documents in the corpus\n",
        "    \"\"\"\n",
        "    # lets keep with the p,q notation above\n",
        "    p = query[None,:].T # take transpose\n",
        "    q = matrix.T # transpose matrix\n",
        "    m = 0.5*(p + q)\n",
        "    return np.sqrt(0.5*(entropy(p,m) + entropy(q,m)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kWpdI7XHJh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_most_similar_documents(query,matrix,k=10):\n",
        "    \"\"\"\n",
        "    This function implements the Jensen-Shannon distance above\n",
        "    and retruns the top k indices of the smallest jensen shannon distances\n",
        "    \"\"\"\n",
        "    sims = jensen_shannon(query,matrix) # list of jensen shannon distances\n",
        "    return sims.argsort()[:k] # the top k positional index of the smallest Jensen Shannon distances"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NprfbW91HL3u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this is surprisingly fast\n",
        "t1 = time.time()\n",
        "most_sim_ids = get_most_similar_documents(new_doc_distribution,doc_topic_dist)\n",
        "t2 = time.time()\n",
        "print(\"Calculation took \",(t2-t1)/60, \"min\")  # Took 5 min"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR7hVj7dH1Tq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "most_sim_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjcPHZSGHRde",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the indices of the four min elements -> returns unsorted\n",
        "min_indexes = get_most_similar_documents(new_doc_distribution,doc_topic_dist)\n",
        "min_indexes = np.array(min_indexes)\n",
        "\n",
        "print(\"my_question             \\n {}\".format(que_question))\n",
        "print(\"similar questions       \\n {}\".format(questions_final[min_indexes]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}