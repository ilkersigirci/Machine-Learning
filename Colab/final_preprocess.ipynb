{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_preprocess.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilkersigirci/ML-with-Colab/blob/master/final_preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7r6YFe0gARUE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#DIFFERENT PREPROCESS\n",
        "def prepare_doc(text): \n",
        "    #print(text) \n",
        "    text = text.lower() \n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, \n",
        "flags=re.MULTILINE) \n",
        "    #print(text) \n",
        "    text = re.sub(r'@\\w+', ' ', text, flags=re.MULTILINE) \n",
        "    text = gensim.parsing.preprocessing.strip_tags(text) \n",
        "    text = gensim.parsing.preprocessing.strip_punctuation(text) \n",
        "    text = gensim.parsing.preprocessing.remove_stopwords(text) \n",
        "    text = gensim.parsing.preprocessing.strip_short(text) \n",
        "    text = gensim.parsing.preprocessing.strip_numeric(text) \n",
        "    text = gensim.parsing.preprocessing.stem_text(text) \n",
        "    #print(text) \n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbTyzA6PgnnC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pprint\n",
        "import tensorflow as tf\n",
        "tf.reset_default_graph()\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NEFC5CU00x-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### ORIGINAL CSV #########################################\n",
        "\n",
        "#pd.set_option('display.max_colwidth', -1) # for viewing questions with full length\n",
        "dfPath = './support_forum_questions.csv'\n",
        "df = pd.read_csv(dfPath, sep='|')\n",
        "df.drop([\"added\", \"login\"], axis = 1,inplace=True)\n",
        "\n",
        "df.dropna(axis = 0, how='any',inplace=True)\n",
        "df.reset_index(drop=True,inplace=True)\n",
        "df[\"id\"] = df[\"id\"].apply(str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cW75X1777bEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################### Glove ############################################## \n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "#!wget http://nlp.stanford.edu/data/glove.840B.300d.zip -P ./\n",
        "#!unzip ./glove.840B.300d.zip -d ./\n",
        "#!touch ./glove_word2vec.txt\n",
        "#_ = glove2word2vec(r'./glove.840B.300d.txt', r\"./glove_word2vec.txt\")\n",
        "\n",
        "glove_model = KeyedVectors.load_word2vec_format(\"./glove_word2vec.txt\", binary=False)\n",
        "#glove_model.save(\"./glove.model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "798UVit2VWQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# just remove html tags\n",
        "\n",
        "def removeHTML(text):\n",
        "   \n",
        "    text = re.sub('<[^<]+?>', ' ',                  text, flags=re.IGNORECASE)  # remove html\n",
        "    text = re.sub('/(\\r\\n)+|\\r+|\\n+|\\t+/i', '' ,    text, flags=re.IGNORECASE)  # remove \\r \\n\n",
        "    text = re.sub(\"nbsp\", \"\",                       text, flags=re.IGNORECASE)  # remove nbsp\n",
        "   \n",
        "    return text\n",
        "\n",
        "df['original'] = df['question'] + ' === ' + df['details']\n",
        "df['original'] = df['original'].apply(removeHTML)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-gTCbBDBMp_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cleaning text\n",
        "import re\n",
        "import nltk\n",
        "from string import punctuation\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# for lemmatizing\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# for stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def clean(text):\n",
        "    \n",
        "    if pd.isnull(text):\n",
        "        return ''\n",
        "    \n",
        "    # Empty question   \n",
        "    if type(text) != str or text=='':\n",
        "        return ''\n",
        "    \n",
        "    #Fixes that are specific to Jotform \n",
        "    text = re.sub('<[^<]+?>', ' ',                  text, flags=re.IGNORECASE)  # remove html\n",
        "    text = re.sub('/(\\r\\n)+|\\r+|\\n+|\\t+/i', '' ,    text, flags=re.IGNORECASE)  # remove \\r \\n\n",
        "    text = re.sub(\"nbsp\", \"\",                       text, flags=re.IGNORECASE)  # remove nbsp\n",
        "    text = re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",     text, flags=re.IGNORECASE)  # remove tags\n",
        "    text = re.sub(\"http\\S*\", \"\",                    text, flags=re.IGNORECASE)  # remove links\n",
        "    text = re.sub(\"www\\S*\", \"\",                     text, flags=re.IGNORECASE)  # remove links\n",
        "    text = re.sub(\"\\S*@\\S*\\s?\", \"\",                 text, flags=re.IGNORECASE)  # remove email\n",
        "    text = re.sub(\"jo[a-z]*form\", \"jotform\",        text, flags=re.IGNORECASE)  # fix1 jotform\n",
        "    text = re.sub(\"jot[a-z]+\", \"jotform\",           text, flags=re.IGNORECASE)  # fix2 jotform\n",
        "    text = re.sub(\"jotform\", \"form\",                text, flags=re.IGNORECASE)  # jotform to form\n",
        "    text = re.sub(\"colour\", \"color\",                text, flags=re.IGNORECASE)  # fix colour\n",
        "    text = re.sub(\"authorizenet\", \"payment\",        text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"calender\", \"calendar\",           text, flags=re.IGNORECASE)\n",
        "    \n",
        "    #Fix some words\n",
        "    text = re.sub(\"autosuspended\", \"auto suspend\",  text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"autoresponses\", \"auto response\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"wordpresscom\", \"word press\",     text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"myforms\", \"my form\",             text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"onedrive\", \"cloud\",              text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"gmailcom\", \"email\",              text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"jo[a-z]*form\", \"form\",           text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"unrequire\", \"not require\",       text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"prepopulate[a-z]*\", \"populate\",  text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"sumbissions\", \"submission\",      text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"submissons\", \"submission\",       text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"mozilla\\S*\", \"browser\",          text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"chrome\\S*\", \"browser\",           text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"android\",\"operating system\",     text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"ios\", \"operating system\",        text, flags=re.IGNORECASE)\n",
        "    \n",
        "    #Fix Negative\n",
        "    text = re.sub(\"n't\", \" not \",                   text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"isnt\", \"is not\",                 text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"arent\", \"are not\",               text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"dont\", \"do not\",                 text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"doesnt\", \"does not\",             text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"didnt\", \"did not\",               text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"shouldnt\", \"should not\",         text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"hasnt\", \"has not\",               text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"havent\", \"have not\",             text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"wont\", \"will not\",               text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"cant\", \"can not\",                text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"couldnt\", \"could not\",           text, flags=re.IGNORECASE)\n",
        "    \n",
        "    #Other fixes\n",
        "    text = re.sub(\"\\'s\", \" \",                       text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"whats\", \"what is\",               text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"\\'ve\", \" have \",                 text, flags=re.IGNORECASE)    \n",
        "    text = re.sub(\"i'm\", \"i am\",                    text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"\\'re\", \" are \",                  text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"\\'d\", \" would \",                 text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"\\'ll\", \" will \",                 text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"e\\.g\\.\", \" example \",            text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"e-mail\", \" email \",              text, flags=re.IGNORECASE)\n",
        "    text = re.sub('\\$', \" dollar \",                 text, flags=re.IGNORECASE)\n",
        "    text = re.sub('\\%', \" percent \",                text, flags=re.IGNORECASE)\n",
        "    text = re.sub('\\&', \" and \",                    text, flags=re.IGNORECASE)\n",
        "    \n",
        "    text = ''.join([c for c in text if c not in punctuation]).lower()           # remove punc\n",
        "    text = re.sub(\" +\",\" \",                         text, flags=re.IGNORECASE)  # remove multiple space\n",
        "    text = re.sub('[^\\x00-\\x7F]+', \"\",              text)                       # remove non-ascii characters    \n",
        "    \n",
        "    ######################### NON-ENGLISH REMOVAL ################################\n",
        "\n",
        "    text = \" \".join(word for word in nltk.wordpunct_tokenize(text) if word.lower() in glove_model.vocab and not word.isdigit())\n",
        "    \n",
        "    \n",
        "    ############################### LEMMATIZE ####################################\n",
        "    \n",
        "    from nltk import word_tokenize, pos_tag\n",
        "    from nltk.stem.wordnet import WordNetLemmatizer\n",
        "    \n",
        "    # n -> noun / v -> verb / a -> adjective / r -> adverb\n",
        "    tokens = word_tokenize(text)\n",
        "    convert_tag = lambda e: ('a' if e[0].lower() == 'j' else e[0].lower()) if e[0].lower() in ['n', 'r', 'v'] else 'n'    \n",
        "    lemmatizer = WordNetLemmatizer()    \n",
        "    text  = ' '.join([ lemmatizer.lemmatize(token, convert_tag(tag)) for token,tag in pos_tag(tokens) ])\n",
        "    \n",
        "    ############################### STOP WORDS ####################################\n",
        "    from nltk.corpus import stopwords\n",
        "    stop_words = stopwords.words('english')\n",
        "    text =  \" \".join(word for word in text.split() if word not in stop_words)\n",
        "\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqtZojMY-cOC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['preprocessed_questions'] = df['question'].apply(clean)\n",
        "df['preprocessed_details']   = df['details'].apply(clean)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wav4MVGurWJd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#detect empty rows\n",
        "df['question'] = df['question'].apply(lambda x: re.sub(\" +\",\" \",x,flags=re.IGNORECASE))\n",
        "df['details' ] = df['details' ].apply(lambda x: re.sub(\" +\",\" \",x,flags=re.IGNORECASE))\n",
        "\n",
        "# drop \"\" and \" \" rows\n",
        "#df.drop(df[condition].index, axis=0, inplace=True)\n",
        "\n",
        "df.drop(df[df['preprocessed_questions'] == \"\" ].index, axis=0, inplace=True)\n",
        "df.drop(df[df['preprocessed_questions'] == \" \"].index, axis=0, inplace=True)\n",
        "df.drop(df[df['preprocessed_details' ]  == \"\" ].index, axis=0, inplace=True)\n",
        "df.drop(df[df['preprocessed_details' ]  == \" \"].index, axis=0, inplace=True)\n",
        "df.reset_index(drop=True,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhCG2ZGhsOhW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['preprocessed_all'] = df['preprocessed_questions'] +' '+ df['preprocessed_details']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUGqJx8gBfUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "df['less8'] = df['all'].apply(lambda x: \" \" if len(x.split()) <= 8 else \"normal\")\n",
        "df.drop(df[df['less8'] == \" \"].index, axis=0, inplace=True)\n",
        "df.reset_index(drop=True,inplace=True)\n",
        "\"\"\"\n",
        "# drop sentences less than 3 words\n",
        "\"\"\"\n",
        "short_indexes = []\n",
        "for index,sentence in enumerate(df['all']):\n",
        "    if(len(sentence.split()) <= 3):\n",
        "        short_indexes.append(index)\n",
        "\n",
        "df.drop(df.index[short_indexes],inplace = True)\n",
        "df.reset_index(drop=True,inplace=True)\n",
        "\"\"\"\n",
        "df = df[df['preprocessed_all'].map( lambda x: len(x.split()) ) > 3]\n",
        "df.reset_index(drop=True,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcY8gqidRT_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# drop non english rows  -> 6888\n",
        "pip install langdetect\n",
        "from langdetect import detect\n",
        "\n",
        "non_english = []\n",
        "for index,sentence in enumerate(df['preprocessed_all']):\n",
        "    try:\n",
        "        if detect(sentence) != \"en\":\n",
        "            non_english.append(index)\n",
        "    except:\n",
        "        non_english.append(index)\n",
        "\n",
        "df.drop(df.index[non_english],inplace = True)\n",
        "df.reset_index(drop=True,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulk26o9yCexY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove rows that contains just stop words  -> 8\n",
        "\"\"\"\n",
        "df['preprocessed_all'] = df['preprocessed_all'].apply( lambda x: \" \".join(word for word in x.split() if word not in stop_words) )\n",
        "indexes = []\n",
        "for index,sentence in enumerate(df['preprocessed_all']):\n",
        "    if sentence == \"\": indexes.append(index)\n",
        "\n",
        "indexes = np.array(indexes)\n",
        "df.drop(df.index[indexes],inplace = True)\n",
        "df.reset_index(drop=True,inplace=True)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inSWwHEFbFS7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv(\"./preprocessed.csv\", sep='|')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}