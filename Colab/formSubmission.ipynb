{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "formSubmission.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilkersigirci/ML-with-Colab/blob/master/formSubmission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIO-ZriA4nID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using numerical indexes -> iloc\n",
        "# using labels as indexes -> loc\n",
        "# df.iloc[0:3]     # for getting rows\n",
        "# df.loc[[0,1,2]]  # for getting rows\n",
        "# df[[\"id\", \"details\"]]  # for getting columns\n",
        "\n",
        "# if conditions\n",
        "# df[ (df[\"id\"] > '70') & (df[\"id\"] < '170') ]\n",
        "\n",
        "# aggregate -> Quantity of sales over each country & Region\n",
        "# df.groupby([\"Country\", \"Region\"])[\"Quantity\"].sum()\n",
        "\n",
        "#replace\n",
        "# df = df.replace(np.nan, 0, regex=True)  # replace nan with 0\n",
        "\n",
        "# str to int\n",
        "# df['DataFrame Column'] = pd.to_numeric(df['DataFrame Column'], errors='coerce')\n",
        "\n",
        "# to str\n",
        "# df[column_name] = df[column_name].apply(str)\n",
        "# df.applymap(str) -> whole frame"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbTyzA6PgnnC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pprint\n",
        "import tensorflow as tf\n",
        "tf.reset_default_graph()\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "#os.chdir(\"/content\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR-60uLhrFM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q kaggle\n",
        "!mkdir -p ~/.kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPDHzfM9rU87",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir mydata\n",
        "!cp drive/My\\ Drive/Colab\\ Notebooks/Dataset/jotform/original.csv  mydata/\n",
        "#!cp drive/My\\ Drive/Colab\\ Notebooks/Dataset/jotform/preprocessed.csv  mydata/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NEFC5CU00x-",
        "colab_type": "code",
        "outputId": "4b39cc56-e260-4897-bc5e-40db05ed3cde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "########################### UNEDITED CSV #########################################\n",
        "\n",
        "!cp drive/My\\ Drive/Colab\\ Notebooks/Dataset/jotform/support_forum_questions.csv  mydata/\n",
        "\n",
        "dfPath = 'mydata/support_forum_questions.csv'\n",
        "original = pd.read_csv(dfPath, sep='|')\n",
        "original.drop([\"added\", \"login\"], axis = 1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3oxMkmc6rC5",
        "colab_type": "code",
        "outputId": "26bd0b17-6911-4762-87c9-bbd5006fdf07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pd.set_option('display.max_colwidth', -1)\n",
        "dfPath = 'mydata/original.csv'\n",
        "#dfPath = 'mydata/preprocessed.csv'\n",
        "df = pd.read_csv(dfPath, sep='|',index_col=0)\n",
        "\n",
        "#axis 0 => axis='rows' / axis='index'\n",
        "#axis 1 => axis='columns'\n",
        "#df.drop([\"added\", \"login\"], axis = 1,inplace=True)\n",
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(219340, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3ZT5y8Hldli",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNykbW0NLH5C",
        "colab_type": "code",
        "outputId": "ea8317cf-4076-47f1-fdb4-59feacad54b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#df = df[pd.notnull(df['id'])]\n",
        "\n",
        "df[\"id\"].isnull().sum() # 2 null"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ng1e4LsZOMGA",
        "colab_type": "code",
        "outputId": "0938dba6-6adb-45b9-973d-5c8f2bf03d3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "df.dropna(axis = 0, how='any',inplace=True)\n",
        "df.reset_index(drop=True,inplace=True)\n",
        "df[\"id\"] = df[\"id\"].apply(str)\n",
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(219598, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pONXzWbCc2f",
        "colab_type": "code",
        "outputId": "a4b75fa8-5417-44e9-bd4e-bc48b730b68f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "count = 0\n",
        "for index in df[\"id\"].values:\n",
        "    if(type(index) != str):\n",
        "        #print(index)\n",
        "        count = count +1\n",
        "    \n",
        "print (count)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Fh2GZoc2CnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import re\n",
        "#p = re.compile(r'<.*?>')\n",
        "#df[\"question\"] = [p.sub(' ', x) for x in df[\"question\"]]\n",
        "#df[\"details\"] = [p.sub(' ', x) for x in df[\"details\"]]\n",
        "\n",
        "#Remove punctuations\n",
        "#df[\"question\"] = [re.sub('[^a-zA-Z1-9]', ' ', x) for x in df[\"question\"]]\n",
        "#df[\"details\"] = [re.sub('[^a-zA-Z1-9]', ' ', x) for x in df[\"details\"]]\n",
        "#remove tags\n",
        "#df[\"question\"] = [re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",x) for x in df[\"question\"]]\n",
        "#df[\"details\"] = [re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",x) for x in df[\"details\"]]\n",
        "#remove multiple spaces\n",
        "#df[\"question\"] = [re.sub(\" +\",\" \",x) for x in df[\"question\"]]\n",
        "#df[\"details\"] = [re.sub(\" +\",\" \",x) for x in df[\"details\"]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i4N0OPn-WRq",
        "colab_type": "code",
        "outputId": "12b74f67-43b2-44de-d1b0-723803d09969",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "#cleaning text\n",
        "import re\n",
        "import nltk\n",
        "from string import punctuation\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "def pad_str(s):\n",
        "    return ' '+s+' '\n",
        "def pad_number(pattern):\n",
        "    matched_string = pattern.group(0)\n",
        "    return pad_str(matched_string)\n",
        "def pad_pattern(pattern):\n",
        "    matched_string = pattern.group(0)\n",
        "    return pad_str(matched_string)\n",
        "\n",
        "def clean(text):\n",
        "    \n",
        "    if pd.isnull(text):\n",
        "        return ''\n",
        "    \n",
        "    # Empty question   \n",
        "    if type(text) != str or text=='':\n",
        "        return ''\n",
        "    \n",
        "    # Clean the text, with the option to stem words.\n",
        "    # stops = set(stopwords.words(\"english\"))\n",
        "    \n",
        "    ###########################################################################################\n",
        "    ################################# SPECIFIC TO JOTFORM #####################################\n",
        "    ###########################################################################################\n",
        "    text = re.sub('<[^<]+?>', ' ',                  text, flags=re.IGNORECASE)  # remove html\n",
        "    text = re.sub('/(\\r\\n)+|\\r+|\\n+|\\t+/i', '' ,    text, flags=re.IGNORECASE)  # remove \\r \\n\n",
        "    text = re.sub(\"nbsp\", \"\",                       text, flags=re.IGNORECASE)  # remove nbsp\n",
        "    text = ''.join([c for c in text if c not in punctuation]).lower()           # remove punc\n",
        "    text = re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",     text, flags=re.IGNORECASE)  # remove tags\n",
        "    text = re.sub(\"http\\S*\", \"\",                    text, flags=re.IGNORECASE)  # remove links\n",
        "    text = re.sub(\"www\\S*\", \"\",                     text, flags=re.IGNORECASE)  # remove links\n",
        "    text = re.sub(\"\\S*@\\S*\\s?\", \"\",                 text, flags=re.IGNORECASE)  # remove email\n",
        "    text = re.sub(\"jo[a-z]*form\", \"jotform\",        text, flags=re.IGNORECASE)  # fix1 jotform\n",
        "    text = re.sub(\"jot[a-z]+\", \"jotform\",           text, flags=re.IGNORECASE)  # fix2 jotform\n",
        "    text = re.sub(\"jotform\", \"form\",                text, flags=re.IGNORECASE)  # jotform to form\n",
        "    text = re.sub(\"colour\", \"color\",                text, flags=re.IGNORECASE)  # fix colour\n",
        "    text = re.sub(\"authorizenet\", \"payment\",        text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"calender\", \"calendar\",           text, flags=re.IGNORECASE)\n",
        "    \n",
        "    text = re.sub(\"autosuspended\", \"auto suspend\",  text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"autoresponses\", \"auto response\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"wordpresscom\", \"word press\",     text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"myforms\", \"my form\",             text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"onedrive\", \"cloud\",              text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"gmailcom\", \"email\",              text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"jo[a-z]*form\", \"form\",           text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"unrequire\", \"not require\",       text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"prepopulate[a-z]*\", \"populate\",  text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"sumbissions\", \"submission\",      text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"submissons\", \"submission\",       text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"mozilla\\S*\", \"browser\",          text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"chrome\\S*\", \"browser\",           text, flags=re.IGNORECASE)\n",
        "    \n",
        "    \n",
        "    ###########################################################################################\n",
        "    ################################# SPECIFIC TO JOTFORM #####################################\n",
        "    ###########################################################################################\n",
        "    \n",
        "    #Fix Negative\n",
        "    text = re.sub(\"n't\", \" not \",                   text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"isnt\", \"is not\",                 text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"arent\", \"are not\",               text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"dont\", \"do not\",                 text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"doesnt\", \"does not\",             text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"didnt\", \"did not\",               text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"shouldnt\", \"should not\",         text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"hasnt\", \"has not\",               text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"havent\", \"have not\",             text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"wont\", \"will not\",               text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"cant\", \"can not\",                text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"couldnt\", \"could not\",           text, flags=re.IGNORECASE)\n",
        "    \n",
        "    text = re.sub(\"\\'s\", \" \",                       text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"whats\", \"what is\",               text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"\\'ve\", \" have \",                 text, flags=re.IGNORECASE)    \n",
        "    text = re.sub(\"i'm\", \"i am\",                    text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"\\'re\", \" are \",                  text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"\\'d\", \" would \",                 text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"\\'ll\", \" will \",                 text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"e\\.g\\.\", \" eg \",                 text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"e-mail\", \" email \",              text, flags=re.IGNORECASE)\n",
        "    text = re.sub('\\$', \" dollar \",                 text, flags=re.IGNORECASE)\n",
        "    text = re.sub('\\%', \" percent \",                text, flags=re.IGNORECASE)\n",
        "    text = re.sub('\\&', \" and \",                    text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"android\",\"operating system\",     text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"ios\", \"operating system\",        text, flags=re.IGNORECASE)\n",
        "    text = re.sub('[0-9]+\\.[0-9]+', \"flt_number\",   text, flags=re.IGNORECASE)  # replace the float numbers with flt_number    \n",
        "    text = re.sub(\" +\",\" \",                         text, flags=re.IGNORECASE)  # remove multiple space\n",
        "    text = re.sub('[^\\x00-\\x7F]+', \"\",              text)                       # remove non-ascii characters\n",
        "    \n",
        "    \n",
        "    #text = re.sub(\"(\\d+)(kK)\", \" \\g<1>000 \",        text, flags=re.IGNORECASE)\n",
        "    #text = re.sub(\"\\(s\\)\", \" \",                     text, flags=re.IGNORECASE)\n",
        "    #text = re.sub(\"[c-fC-F]\\:\\/\", \" disk \",         text, flags=re.IGNORECASE)\n",
        "    #text = re.sub(\"(?<=[0-9])rs \", \" rs \",          text, flags=re.IGNORECASE)\n",
        "    #text = re.sub(\" rs(?=[0-9])\", \" rs \",           text, flags=re.IGNORECASE)\n",
        "    #text = re.sub(r\" \\0rs \", \" rs \",                text, flags=re.IGNORECASE)    \n",
        "    #text = re.sub('[^\\x00-\\x7F]+', pad_str(SPECIAL_TOKENS['non-ascii']), text) # replace non-ascii characters with non_ascii_word\n",
        "    #text =re.sub('(?<=[0-9])\\,(?=[0-9])',\"\",text, flags=re.IGNORECASE)         # remove comma between numbers\n",
        "    #text = re.sub('[\\!\\?\\@\\^\\+\\*\\/\\,\\~\\|\\`\\=\\:\\;\\.\\#\\\\\\]', pad_pattern, text)        \n",
        "    #text = re.sub(\"(the[\\s]+|The[\\s]+)?U\\.S\\.A\\.\", \" America \", text, flags=re.IGNORECASE)\n",
        "    #text = re.sub(\"(the[\\s]+|The[\\s]+)?United State(s)?\", \" America \", text, flags=re.IGNORECASE)    \n",
        "    # text = re.sub('[0-9]+', pad_number,  text, flags=re.IGNORECASE)           # all numbers should separate from words, this is too aggressive   \n",
        "\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqtZojMY-cOC",
        "colab_type": "code",
        "outputId": "9c5f515b-fd0d-42b5-9803-3ed3c0a62764",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "t1 = time.time()\n",
        "df['question'] = df['question'].apply(clean)\n",
        "t2 = time.time()\n",
        "print(\"Calculation took \",(t2-t1)/60, \"min\")  # Took 0.3 min\n",
        "t1 = time.time()\n",
        "df['details'] = df['details'].apply(clean)\n",
        "t2 = time.time()\n",
        "print(\"Calculation took \",(t2-t1)/60, \"min\")  # Took 3 min"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculation took  0.3677972912788391 min\n",
            "Calculation took  3.275190814336141 min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cW75X1777bEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################### Glove ############################################## \n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "#!wget http://nlp.stanford.edu/data/glove.840B.300d.zip -P mydata/\n",
        "#!unzip mydata/glove.840B.300d.zip -d mydata/\n",
        "#!cp drive/My\\ Drive/Colab\\ Notebooks/Dataset/glove.840B.300d.txt mydata/\n",
        "#!touch mydata/glove_word2vec.txt\n",
        "#_ = glove2word2vec(r'mydata/glove.840B.300d.txt', r\"mydata/glove_word2vec.txt\")\n",
        "\n",
        "!cp  drive/My\\ Drive/Colab\\ Notebooks/Dataset/glove_word2vec.txt mydata/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU9zCjDy-cdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "t1 = time.time()\n",
        "glove_model = KeyedVectors.load_word2vec_format(\"mydata/glove_word2vec.txt\", binary=False)\n",
        "t2 = time.time()\n",
        "print(\"Calculation took \",(t2-t1)/60, \"min\")  # Took 11 min"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyfiUYyEr55q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glove_model.save(\"mydata/glove.model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LS7hFVqjwu_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#noneng bulma vardi\n",
        "from collections import Counter\n",
        "c=Counter(noneng)\n",
        "sorted_c = sorted(c.items(), key=lambda kv: kv[1],reverse=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HM_914FLCJSx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fix_words(text):\n",
        "    text = re.sub(\"autosuspended\", \"auto suspend\",  text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"autoresponses\", \"auto response\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"wordpresscom\", \"word press\",     text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"myforms\", \"my form\",             text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"onedrive\", \"cloud\",              text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"gmailcom\", \"email\",              text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"jo[a-z]*form\", \"form\",           text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"unrequire\", \"not require\",       text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"prepopulate[a-z]*\", \"populate\",  text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"sumbissions\", \"submission\",      text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"submissons\", \"submission\",       text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"mozilla\\S*\", \"browser\",          text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"chrome\\S*\", \"browser\",           text, flags=re.IGNORECASE)\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01_IgH_8CTxW",
        "colab_type": "code",
        "outputId": "1589da47-9d5f-4f70-df9c-092350eb0494",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "t1 = time.time()\n",
        "df['question'] = df['question'].apply(fix_words)\n",
        "t2 = time.time()\n",
        "print(\"Calculation took \",(t2-t1)/60, \"min\")  # Took 0.06 min\n",
        "t1 = time.time()\n",
        "df['details'] = df['details'].apply(fix_words)\n",
        "t2 = time.time()\n",
        "print(\"Calculation took \",(t2-t1)/60, \"min\")  # Took 0.2 min"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculation took  0.07739330530166626 min\n",
            "Calculation took  0.23662294944127402 min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-rdO8ccfjRf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "#nltk.download('wordnet')\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('averaged_perceptron_tagger')\n",
        "#nltk.download('stopwords')\n",
        "#from nltk.corpus import stopwords\n",
        "#stop_words = stopwords.words('english')\n",
        "\n",
        "def preprocess(text):\n",
        "    \n",
        "    ######################### NON-ENGLISH REMOVAL ################################\n",
        "\n",
        "    text = \" \".join(word for word in nltk.wordpunct_tokenize(text) if word.lower() in glove_model.vocab and not word.isdigit())\n",
        "   \n",
        "    ############################### LEMMATIZE ####################################\n",
        "    \n",
        "    from nltk import word_tokenize, pos_tag\n",
        "    from nltk.stem.wordnet import WordNetLemmatizer\n",
        "    #nltk.download('wordnet')\n",
        "    #nltk.download('punkt')\n",
        "    #nltk.download('averaged_perceptron_tagger')\n",
        "    \n",
        "    #no tag\n",
        "    #text  = ' '.join([ lemmatizer.lemmatize(word) for word in text.split() ])\n",
        "    \n",
        "    # n -> noun / v -> verb / a -> adjective / r -> adverb\n",
        "    tokens = word_tokenize(text)\n",
        "    convert_tag = lambda e: ('a' if e[0].lower() == 'j' else e[0].lower()) if e[0].lower() in ['n', 'r', 'v'] else 'n'    \n",
        "    lemmatizer = WordNetLemmatizer()    \n",
        "    text  = ' '.join([ lemmatizer.lemmatize(token, convert_tag(tag)) for token,tag in pos_tag(tokens) ])   \n",
        "    \n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPZrzwh8vtEn",
        "colab_type": "code",
        "outputId": "d8bab042-5d39-4750-d4e4-57a477ef83ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "t1 = time.time()\n",
        "#test_q = df['question'].apply(preprocess)\n",
        "t2 = time.time()\n",
        "print(\"Calculation took \",(t2-t1)/60, \"min\")  # Took 3 min\n",
        "t1 = time.time()\n",
        "test_d = df['details'].apply(preprocess)\n",
        "t2 = time.time()\n",
        "print(\"Calculation took \",(t2-t1)/60, \"min\")  # Took 11 min"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculation took  3.933906555175781e-07 min\n",
            "Calculation took  11.558749902248383 min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nay69vL0ojp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count = 0 #135 / 127\n",
        "indexes = []\n",
        "for index,sentence in enumerate(df['question']):\n",
        "    if sentence == \"\":\n",
        "        indexes.append(index)\n",
        "        count = count + 1\n",
        "\n",
        "indexes = np.array(indexes)\n",
        "print(count)\n",
        "#print(df[\"details\"][df.index[indexes]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFZ8uZP2Fsb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['question'] = test_q\n",
        "df['details'] = test_d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wav4MVGurWJd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#detect empty rows\n",
        "df['question'] = df['question'].apply(lambda x: re.sub(\" +\",\" \",x,flags=re.IGNORECASE))\n",
        "df['details' ] = df['details' ].apply(lambda x: re.sub(\" +\",\" \",x,flags=re.IGNORECASE))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_EFUxAx6JFw",
        "colab_type": "code",
        "outputId": "f9ef78f5-6529-4c2e-c07b-5cb63cef3eb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# drop \"\" and \" \" rows\n",
        "#df.drop(df[condition].index, axis=0, inplace=True)\n",
        "\n",
        "#question -> 69 / 158\n",
        "#details  -> 1  / 1340\n",
        "\n",
        "df.drop(df[df['question'] == \"\" ].index, axis=0, inplace=True)\n",
        "df.drop(df[df['question'] == \" \"].index, axis=0, inplace=True)\n",
        "df.drop(df[df['details' ] == \"\" ].index, axis=0, inplace=True)\n",
        "df.drop(df[df['details' ] == \" \"].index, axis=0, inplace=True)\n",
        "df.reset_index(drop=True,inplace=True)\n",
        "\n",
        "count = 0\n",
        "for sentence in df['details']:\n",
        "    if sentence == \" \":\n",
        "        count = count + 1\n",
        "        \n",
        "print(count)\n",
        "print(df.shape) #219340"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "(219340, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhCG2ZGhsOhW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['all'] = df['question'] +' '+ df['details']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaqiyjWG2Obe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head(20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUGqJx8gBfUN",
        "colab_type": "code",
        "outputId": "c4c41d22-0557-42b8-ae3c-5f67b79755fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "df['less8'] = df['all'].apply(lambda x: \" \" if len(x.split()) <= 8 else \"normal\")\n",
        "df.drop(df[df['less8'] == \" \"].index, axis=0, inplace=True)\n",
        "df.reset_index(drop=True,inplace=True)\n",
        "\"\"\"\n",
        "# drop sentences less than 3 words\n",
        "short_indexes = []\n",
        "for index,sentence in enumerate(df['all']):\n",
        "    if(len(sentence.split()) <= 3):\n",
        "        short_indexes.append(index)\n",
        "\n",
        "df.drop(df.index[short_indexes],inplace = True)\n",
        "df.reset_index(drop=True,inplace=True)\n",
        "\n",
        "count = 0        # normalde 5087\n",
        "for sentence in df['all']:\n",
        "    if(len(sentence.split()) <= 8):\n",
        "        count = count + 1\n",
        "\n",
        "print(count)\n",
        "print(df.shape) #222518"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4337\n",
            "(226672, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcY8gqidRT_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# drop non english rows  -> 6888\n",
        "!pip install langdetect\n",
        "from langdetect import detect\n",
        "\n",
        "t1 = time.time()\n",
        "\n",
        "non_english = []\n",
        "sen = []\n",
        "for index,sentence in enumerate(df['all']):\n",
        "    try:\n",
        "        if detect(sentence) != \"en\":\n",
        "            sen.append(sentence)\n",
        "            non_english.append(index)\n",
        "    except:\n",
        "        sen.append(sentence)\n",
        "        non_english.append(index)\n",
        "        \n",
        "t2 = time.time()\n",
        "print(\"Calculation took \",(t2-t1)/60, \"min\")  # Took ? min \n",
        "\n",
        "df.drop(df.index[non_english],inplace = True)\n",
        "df.reset_index(drop=True,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulk26o9yCexY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove rows that contains just stop words  -> 8\n",
        "\n",
        "test_all = df['all'].apply( lambda x: \" \".join(word for word in x.split() if word not in stop_words) )\n",
        "count = 0\n",
        "indexes = []\n",
        "for index,sentence in enumerate(test_all):\n",
        "    if sentence == \"\":\n",
        "        indexes.append(index)\n",
        "        count = count + 1\n",
        "\n",
        "indexes = np.array(indexes)\n",
        "print(count)\n",
        "print(df[\"all\"][df.index[indexes]])\n",
        "\n",
        "df.drop(df.index[indexes],inplace = True)\n",
        "df.reset_index(drop=True,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inSWwHEFbFS7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv(\"mydata/original.csv\", sep='|')\n",
        "!cp mydata/original.csv drive/My\\ Drive/Colab\\ Notebooks/Dataset/jotform/  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ncND5MnEUKe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove stop_words\n",
        "df['all'] = df['all'].apply( lambda x: \" \".join(word for word in x.split() if word not in stop_words) )\n",
        "\n",
        "df.to_csv(\"mydata/preprocessed.csv\", sep='|')\n",
        "!cp mydata/preprocessed.csv drive/My\\ Drive/Colab\\ Notebooks/Dataset/jotform/  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMM1Y3kkis1_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################################################################################\n",
        "##################################### End of Preprocess ########################################\n",
        "################################################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDyZs9QhGUkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "questions = df['question']\n",
        "questions = np.array(questions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sadEieqgR9PL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################################################################################\n",
        "################################ Word Mover's Distance (WMD) ###################################\n",
        "################################################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHYzQtdSR_jC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Word Mover's Distance (WMD)\n",
        "\n",
        "#Copy GoogleNews-vectors-negative300.bin.gz\n",
        "!cp  drive/My\\ Drive/Colab\\ Notebooks/Dataset/quora/GoogleNews-vectors-negative300.bin.gz mydata/\n",
        "\n",
        "import gensim \n",
        "googlenews_corpus_path = 'mydata/GoogleNews-vectors-negative300.bin.gz'\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(googlenews_corpus_path, binary=True)\n",
        "model.init_sims(replace=True) # normalize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soadbCPnr4Db",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train my word2vec model\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "# split sentences\n",
        "\n",
        "#test = df.question.apply(lambda x: len(str(x)))\n",
        "splitted = []\n",
        "for sentence in questions:\n",
        "    if type(sentence) == str:\n",
        "        splitted.append(sentence.lower().split())\n",
        "\n",
        "splitted = np.array(splitted)\n",
        "\n",
        "# size      -> determines the dimensions of the word vectors\n",
        "# window    -> number of forward and backward words used in training\n",
        "# min_count -> number of minimum times a word needs to appear to be included in the training\n",
        "my_model = Word2Vec(splitted, size = 10, window = 5, min_count = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZUoEjUBA82R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "def my_wmd(q1, q2,model_type):\n",
        "    q1 = str(q1).lower().split()\n",
        "    q2 = str(q2).lower().split()\n",
        "    q1 = [w for w in q1 if w not in stop_words]\n",
        "    q2 = [w for w in q2 if w not in stop_words]\n",
        "    return model_type.wmdistance(q1, q2)\n",
        "\n",
        "#all_except_index = np.delete(a, ind, axis=0) \n",
        "\n",
        "my_question = \"how do I export data to google drive\"\n",
        "distance = []\n",
        "for question in questions:\n",
        "    distance.append(my_wmd(my_question,question,my_model))\n",
        "\n",
        "distance = np.array(distance)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7TLeU-zvlBz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "distance"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VSPgDEnUr6m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the indices of the four min elements -> returns unsorted\n",
        "min_indexes = np.argpartition(distance, 4)[:4]\n",
        "#sort it\n",
        "min_indexes = min_indexes[np.argsort(distance[min_indexes])]\n",
        "\n",
        "print(\"my_question       -> {}\".format(my_question))\n",
        "print(\"similar questions -> {}\".format(original_questions[min_indexes]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQw05P3xKkr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count = 0\n",
        "for sentence in splitted:\n",
        "    if(len(sentence) > 3):\n",
        "        count = count +1\n",
        "\n",
        "print(count)\n",
        "\n",
        "for sentence in splitted:\n",
        "    for word in sentence:\n",
        "        if word == \"jotform\":\n",
        "            count = count + 1\n",
        "print(count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JE7lHeD_ufDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "#nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "#remove stopwords\n",
        "\n",
        "#for i in range(len(splitted)):\n",
        "#    splitted[i] = [word for word in splitted ]\n",
        "\n",
        "count  = 0\n",
        "nonenglish = []\n",
        "for sentence in splitted:\n",
        "    for word in sentence:\n",
        "        if word not in model.vocab and word not in stop_words and word != \"jotform\":\n",
        "            count = count + 1\n",
        "            nonenglish.append(word)\n",
        "            \n",
        "\n",
        "print(count)\n",
        "print(nonenglish)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCpFU9swSLrx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training the model with list of sentences (with 4 CPU cores)\n",
        "# model.train(sentences, workers=4)\n",
        "\n",
        "#add word to GoogleNews model\n",
        "\n",
        "print('jotform' in model.vocab)\n",
        "\n",
        "# add jotform\n",
        "model.add('jotform',model.get_vector('company'),replace=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF7D8xekOkvd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################################################################################\n",
        "############################ Latent Dirichlet Allocation (LDA) #################################\n",
        "################################################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEyN7VJUQODi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "import numpy as np\n",
        "np.random.seed(2018)\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkUW4bzERJAK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfPath = 'mydata/preprocessed.csv'\n",
        "data = pd.read_csv(dfPath, sep='|') # ,dtype = str\n",
        "data = data.fillna(\" \")\n",
        "data_questions = data[[\"question\"]]\n",
        "data_questions['index'] = data_questions.index\n",
        "questions = data_questions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lf7YeQB9SS8s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "# TODO : burayi duzenle sonradan\n",
        "def lemmatize_stemming(text):\n",
        "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
        "\n",
        "def preprocess(text):\n",
        "    result = []\n",
        "    for token in gensim.utils.simple_preprocess(text):\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2:\n",
        "            result.append(lemmatize_stemming(token))\n",
        "        \n",
        "    return result\n",
        "\n",
        "processed_questions = questions['question'].map(preprocess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PsxicMread6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################ Bag of words on the dataset ############################\n",
        "\n",
        "dictionary = gensim.corpora.Dictionary(processed_questions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5tO-A-AedkH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count = 0\n",
        "for k, v in dictionary.iteritems():\n",
        "    print(k, v)\n",
        "    count += 1\n",
        "    if count > 10:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aa3fTzy4eygJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#less than 15 documents (absolute number) or\n",
        "#more than 0.5 documents (fraction of total corpus size, not absolute number).\n",
        "#keep only the first 100000 most frequent tokens.\n",
        "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cqLus8We_nT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_questions]\n",
        "bow_corpus[4310]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTSR7GUvfNQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bow_doc_4310 = bow_corpus[4310]\n",
        "\n",
        "for i in range(len(bow_doc_4310)):\n",
        "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
        "                                                     dictionary[bow_doc_4310[i][0]], \n",
        "                                                     bow_doc_4310[i][1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzRzetqEfcpq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####################### TF-IDF ############################\n",
        "\n",
        "from gensim import corpora, models\n",
        "\n",
        "tfidf = models.TfidfModel(bow_corpus)\n",
        "corpus_tfidf = tfidf[bow_corpus]\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "for doc in corpus_tfidf:\n",
        "    pprint(doc)\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndVv6qPmfqaW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########### Running LDA using Bag of Words #################\n",
        "\n",
        "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
        "\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rdmaDtCf4Oq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########### Running LDA using TF-IDF #################\n",
        "\n",
        "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
        "\n",
        "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
        "    print('Topic: {} Word: {}'.format(idx, topic))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cw14_hznf7xz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Performance Evaluation \n",
        "print(processed_questions[4310])\n",
        "print(\"##########################################################################\\\n",
        "################################################################################# \")\n",
        "# Bag of Words\n",
        "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
        "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))\n",
        "\n",
        "print(\"##########################################################################\\\n",
        "################################################################################# \")\n",
        "print(\"##########################################################################\\\n",
        "################################################################################# \")\n",
        "# Tf-idf\n",
        "for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
        "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUtIZnuViQ9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### Testing Model ####\n",
        "\n",
        "unseen_document = 'how do I export data to google drive'\n",
        "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
        "\n",
        "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
        "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJNjgVvc5kRb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################## OTHER METHODS #######################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYKGPTdO0G4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "documents = questions\n",
        "#change nan values to empty string\n",
        "count = 0\n",
        "for index, sentence in enumerate(documents):\n",
        "    if type(sentence) == float:\n",
        "        documents[index] = \" \"\n",
        "        count = count + 1\n",
        "        \n",
        "print(count)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}